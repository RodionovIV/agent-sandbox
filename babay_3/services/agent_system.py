from langchain.agents import create_react_agent\nfrom langchain.agents.agent_toolkits import GigaChat\nfrom langchain.memory import MemorySaver\nfrom langchain.schema.messages import HumanMessage\nfrom langchain.tools import giga_tool\nfrom langchain.tools.base import BaseTool\nfrom langchain.schema import BaseModel as LangchainBaseModel\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain.chains import RetrievalQA\nfrom typing import List, Dict, Any\n\nclass AgentSystem:\n    def __init__(self):\n        self.llm = GigaChat(\n            model="GigaChat-2-Max",\n            verify_ssl_certs=False,\n            profanity_check=False,\n            streaming=False,\n            max_tokens=8192,\n            temperature=0.3,\n            repetition_penalty=1.01,\n            timeout=60\n        )\n        self.functions = [rag_search]\n        self.llm_with_functions = self.llm.bind_tools(self.functions)\n        self.agent_executor = create_react_agent(self.llm_with_functions, self.functions, checkpointer=MemorySaver())\n\n    def process(self, query: str) -> str:\n        message = {"messages": [HumanMessage(content=query)]}\n        result = self.agent_executor.invoke(message)\n        return result["result"]\n\nclass RagTool:\n    def __init__(self):\n        llm = GigaChat(\n            model="GigaChat-2-Max",\n            verify_ssl_certs=False,\n            profanity_check=False,\n            streaming=False,\n            max_tokens=8192,\n            temperature=0.3,\n            repetition_penalty=1.01,\n            timeout=60\n        )\n        example_docs = [\n            "Погода в МСК пасмурная",\n            "Слон купил велосипед",\n            "Bombini Gussini la brateelo Bombordiro Crocodilo"\n        ]\n        embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")\n        docs = [Document(page_content=text) for text in example_docs]\n        vectorstore = FAISS.from_documents(docs, embedding)\n        retriver = vectorstore.as_retriever()\n\n        self.rag_chain = RetrievalQA.from_chain_type(\n            llm=llm,\n            retriever=retriever,\n            return_source_documents=True\n        )\n\n    def run_tool(self, query):\n        _query = {"query": query}\n        result = self.rag_chain.invoke(_query)["result"]\n        return result\n\nclass RagResult(LangchainBaseModel):\n    status: str = Field(description="Статус исполнения RAG")\n    message: str = Field(description="Сообщение о результате исполнения RAG")\n    result: str = Field(description="Результат исполнения RAG")\n\nfew_shot_examples_rag = [\n    {\n        "request": "Сколько лет Льву Николаевичу Толстому?",\n        "params": {"query": "Сколько лет Льву Николаевичу Толстому?"},\n    }\n]\n\n@giga_tool(few_shot_examples=few_shot_examples_rag)\ndef rag_search(\n    query: str = Field(description="Запрос в векторную БД для RAG")\n) -> RagResult:\n    """Использование поиска"""\n    print(f"! rag_search with query: {query}")\n    try:\n        result = rag_chain.run_tool(query)\n        return RagResult(status="OK", message="Ответ получен!", result=result)\n    except Exception as e:\n        return RagResult(status="FAIL", message=f"Не удалось запустить инструмент, ошибка: {e}", result=None)