from langchain_gigachat.chat_models import GigaChat\nfrom langchain_gigachat.tools.giga_tool import giga_tool\nfrom langchain.schema import HumanMessage, SystemMessage, Document, AIMessage\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import TypedDict, Optional, Literal, List, Dict\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel, Field\nfrom pathlib import Path\nimport time\nimport datetime\nimport warnings\nimport os\nimport re\n\n\nclass RagTool:\n    def __init__(self):\n        llm = GigaChat(\n            model="GigaChat-2-Max",\n            verify_ssl_certs=False,\n            profanity_check=False,\n            streaming=False,\n            max_tokens=8192,\n            temperature=0.3,\n            repetition_penalty=1.01,\n            timeout=60\n        )\n        example_docs = [\n            "Погода в МСК пасмурная",\n            "Слон купил велосипед",\n            "Bombini Gussini la brateelo Bombordiro Crocodilo"\n        ]\n        embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")\n        docs = [Document(page_content=text) for text in example_docs]\n        vectorstore = FAISS.from_documents(docs, embedding)\n        retriver = vectorstore.as_retriever()\n\n        self.rag_chain = RetrievalQA.from_chain_type(\n            llm=llm,\n            retriever=retriever,\n            return_source_documents=True\n        )\n\n    def run_tool(self, query):\n        _query = {"query": query}\n        result = self.rag_chain.invoke(_query)["result"]\n        return result\n\n\nclass RagResult(BaseModel):\n    status: str = Field(description="Статус исполнения RAG")\n    message: str = Field(description="Сообщение о результате исполнения RAG")\n    result: str = Field(description="Результат исполнения RAG")\n\n\nfew_shot_examples_rag = [\n    {\n        "request": "Сколько лет Льву Николаевичу Толстому?",\n        "params": {"query": "Сколько лет Льву Николаевичу Толстому?"},\n    }\n]\n\n\n@giga_tool(few_shot_examples=few_shot_examples_rag)\ndef rag_search(\n    query: str = Field(description="Запрос в векторную БД для RAG")\n) -> RagResult:\n    """Использование поиска"""\n    print(f"! rag_search with query: {query}")\n    try:\n        result = rag_chain.run_tool(query)\n        return RagResult(status="OK", message="Ответ получен!", result=result)\n    except Exception as e:\n        return RagResult(status="FAIL", message=f"Не удалось запустить инструмент, ошибка: {e}", result=None)\n\n\nllm = GigaChat(\n    model="GigaChat-2-Max",\n    verify_ssl_certs=False,\n    profanity_check=False,\n    streaming=False,\n    max_tokens=8192,\n    temperature=0.3,\n    repetition_penalty=1.01,\n    timeout=60\n)\n\nrag_chain = RagTool()\n\nfunctions = [rag_search]\nllm_with_functions = llm.bind_tools(functions)\nagent_executor = create_react_agent(llm_with_functions, functions, checkpointer=MemorySaver())\n\n\nclass AgentSystem:\n    def process(self, query: str) -> str:\n        config = {"configurable": {"thread_id": "thread_id4"}}\n        message = {"messages": [HumanMessage(content=query)]}\n        result = agent_executor.invoke(message, config=config)\n        return result